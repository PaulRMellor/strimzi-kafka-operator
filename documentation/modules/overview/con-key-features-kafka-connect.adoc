// This module is included in:
//
// overview/assembly-key-features.adoc

[id="key-features-kafka-connect_{context}"]
= Kafka Connect concepts

[role="_abstract"]
Strimzi operates Kafka Connect in _distributed mode_, distributing data streaming tasks across worker nodes.
A Kafka Connect cluster comprises a group of worker nodes that have the same group id, which must be unique to that cluster.
The group id is assigned in the worker configuration through the `KafkaConnect` resource.
The worker configuration must also specify the names of internal Kafka Connect topics that the workers will use.
The internal topics are created by Kafka connect to store connector offset and status information.
The names of the topics must also be unique to the Kafka Connect cluster.

.Kafka Connect in distributed mode
image:overview/kafka-concepts-kafka-connect.png[Kafka Connect interaction in distributed mode]

Kafka Connect loads connector instances on start up and distributes data streaming tasks and connector configuration across the worker nodes.
Workers run the data streaming processes for the connector instances.
Each worker runs as a separate node to make the Kafka Connect cluster more fault tolerant.
Work is distributed evenly through load balancing and is split into tasks.
If there are more tasks than workers, workers are assigned multiple tasks.
If a worker fails, its processes are automatically assigned to active workers in the Kafka Connect cluster.

== Connectors

Connectors copy data from external systems to Kafka or from Kafka into external systems.

* Source connectors push data into Kafka.
* Sink connectors extract data out of Kafka.

Connector plugins provide the implementation for Kafka Connect to run connector instances.
Connector instances create and orchestrate the tasks required to transfer data in and out of Kafka.
Tasks split the work required.
The number of tasks created relates to the number of Kafka topics and partitions.

[NOTE]
--
Kafka provides two built-in connectors:

* `FileStreamSourceConnector` streams data from an external system to Kafka, reading lines from an input source and sending each line to a Kafka topic.
* `FileStreamSinkConnector` streams data from Kafka to an external system, reading messages from a Kafka topic and creating a line for each in an output file.
--

== Workers

Workers are individual pods in a Kubernetes cluster.
A Kafka Connect cluster represents a group of workers with the same group id.
The workers are assigned one or more connector instances and tasks.
Workers perform processes for the connectors and tasks.

The distributed approach to deploying Kafka Connect is fault tolerant and scalable.
If a worker node fails, active workers share the tasks.
You can add to a group of worker nodes through configuration of the `replicas` property in the `KafkaConnect` resource.

Workers store the initial connector configuration deployed to the Kafka Connect cluster.
They also start connectors and their tasks.

* Source connector workers read events from an external data system and use converters to construct key and value pairs to store in Kafka.
* Sink connector workers read records from Kafka and use converters to construct records to pass into an external data system.

== Transforms

Kafka Connect translates and transforms external data.
Single-message transforms change messages into a format suitable for the target destination.
For example, a transform might insert or rename a field. Transforms can also filter and route data.
Connector plugins contain the implementation required for workers to perform one or more transformations.

* Source connectors transform data before converting it into a format supported by Kafka.
* Sink connectors transform data after converting it into a format suitable for an external data system.

Kafka Connect provides a set of standard transforms, but you can also create your own.

== Converters

When a worker receives data, it converts the data into an appropriate format using a converter.
You specify converters for workers in the worker `config` in the `KafkaConnect` resource.

Kafka Connect can convert data to and from formats supported by Kafka, such as JSON or Avro.
It also supports schemas for structuring data.
If you are not converting data into a structured format, you donâ€™t need to enable schemas.

NOTE: You can also specify converters for specific connectors to override the general Kafka Connect worker configuration that applies to all workers.

== Tasks

Data transfer orchestrated by connector instances is split into tasks that run in parallel.
A task is started using the configuration for the connector instance.
Kafka Connect distributes tasks to workers.

* A source connector task polls the external data system and returns a list of records that a worker sends to the Kafka brokers.
* A sink connector task receives Kafka records from a worker for writing to the external data system.

If a task fails, it can be restarted through the Kafka Connect API.

NOTE: You can control the maximum number of tasks that can run in parallel by setting `tasksMax` in the connector configuration.

[role="_additional-resources"]
.Additional resources

* link:{BookURLUsing}#property-kafka-connect-config-reference[Kafka Connect configuration^] of workers
