// This module is included in:
//
// overview/assembly-key-features.adoc

[id="key-features-kafka-connect_{context}"]
= Kafka Connect concepts

[role="_abstract"]
Strimzi operates Kafka Connect in _distributed mode_, distributing data streaming tasks across worker pods.
A Kafka Connect cluster comprises a group of worker pods that have the same group id, which must be unique to that cluster.
The group id is assigned in the worker configuration through the `KafkaConnect` resource.
The worker configuration must also specify the names of internal Kafka Connect topics that the workers will use.
The internal topics are created by Kafka Connect to store connector offset and status information.
The names of the topics must also be unique to the Kafka Connect cluster.

.Kafka Connect in distributed mode
image:overview/kafka-concepts-kafka-connect.png[Kafka Connect interaction in distributed mode]

Kafka Connect loads existing connector instances on start up and distributes data streaming tasks and connector configuration across the worker pods.
Workers run the data streaming processes for the connector instances.
Each worker runs as a separate node to make the Kafka Connect cluster more fault tolerant.
Work is distributed evenly through load balancing and is divided into tasks.
If there are more tasks than workers, workers are assigned multiple tasks.
If a worker fails, its tasks are automatically assigned to active workers in the Kafka Connect cluster.

== Connectors

Connectors copy data from external systems to Kafka, or from Kafka into external systems.

* Source connectors push data into Kafka.
* Sink connectors extract data out of Kafka.

Connector plugins provide the implementation for Kafka Connect to run connector instances.
Connector instances create the tasks required to transfer data in and out of Kafka.
The Kafka Connect runtime orchestrates the tasks to split the work required between the worker pods.

NOTE: MirrorMaker 2.0 also uses the Kafka Connect framework.
In this case, the external data system is another Kafka cluster.
Specialized connectors for MirrorMaker 2.0 manage data replication between source and target Kafka clusters.

[NOTE]
--
Kafka provides two built-in connectors:

* `FileStreamSourceConnector` streams data from an external system to Kafka, reading lines from an input source and sending each line to a Kafka topic.
* `FileStreamSinkConnector` streams data from Kafka to an external system, reading messages from a Kafka topic and creating a line for each in an output file.
--

== Workers

Workers are individual nodes in a Kubernetes cluster.
A Kafka Connect cluster contains a group of workers with the same group id.
The workers are assigned one or more connector instances and tasks.
Workers perform processes for the connectors and tasks.

The distributed approach to deploying Kafka Connect is fault tolerant and scalable.
If a worker pod fails, active workers share the tasks.
You can add to a group of worker pods through configuration of the `replicas` property in the `KafkaConnect` resource.

Workers store the initial connector configuration deployed to the Kafka Connect cluster.
They also start connectors and their tasks.

* Source connector workers read events from an external data system and use converters to construct key and value pairs to store in Kafka.
* Sink connector workers read records from Kafka and use converters to construct records to pass into an external data system.

== Transforms

Kafka Connect translates and transforms external data.
Single-message transforms change messages into a format suitable for the target destination.
For example, a transform might insert or rename a field. Transforms can also filter and route data.
Connector plugins contain the implementation required for workers to perform one or more transformations.

* Source connectors transform data before converting it into a format supported by Kafka.
* Sink connectors transform data after converting it into a format suitable for an external data system.

A transform comprises a set of Java class files packaged in a JAR file for inclusion in a connector plugin.
Kafka Connect provides a set of standard transforms, but you can also create your own.

== Converters

When a worker receives data, it converts the data into an appropriate format using a converter.
You specify converters for workers in the worker `config` in the `KafkaConnect` resource.

Kafka Connect can convert data to and from formats supported by Kafka, such as JSON or Avro.
It also supports schemas for structuring data.
If you are not converting data into a structured format, you donâ€™t need to enable schemas.

NOTE: You can also specify converters for specific connectors to override the general Kafka Connect worker configuration that applies to all workers.

== Tasks

Data transfer orchestrated by the Kafka Connect runtime is split into tasks that run in parallel.
A task is started using the configuration supplied by a connector instance.
Kafka Connect distributes tasks to workers.

* A source connector task polls the external data system and returns a list of records that a worker sends to the Kafka brokers.
* A sink connector task receives Kafka records from a worker for writing to the external data system.

If a task fails, it can be restarted through the Kafka Connect API.

For sink connectors, the number of tasks created relates to the number of partitions being consumed.
For source connectors, how the source data is partitioned is defined by the connector.
You can control the maximum number of tasks that can run in parallel by setting `tasksMax` in the connector configuration.

[role="_additional-resources"]
.Additional resources
* http://kafka.apache.org[Apache Kafka documentation^]
* link:{BookURLUsing}#property-kafka-connect-config-reference[Kafka Connect configuration^] of workers
* link:{BookURLUsing}#proc-mirrormaker-replication-str[Synchronizing data between Kafka clusters using MirrorMaker 2.0^]
