// This module is included in:
//
// overview/assembly-key-features.adoc

[id="key-features-kafka-connect_{context}"]
= Kafka Connect concepts

[role="_abstract"]
Kafka Connect loads existing connector instances on start up and distributes data streaming tasks and connector configuration across worker pods.
Workers run the tasks for the connector instances.
Each worker runs as a separate pod to make the Kafka Connect cluster more fault tolerant.
Work is distributed evenly through load balancing and is divided into tasks.
If there are more tasks than workers, workers are assigned multiple tasks.
If a worker fails, its tasks are automatically assigned to active workers in the Kafka Connect cluster.

The following diagram shows the main Kafka Connect components used in streaming data:

* Connectors to create tasks
* Tasks to split the work
* Workers to start tasks
* Transforms to manipulate data
* Converters to convert data

.Workers running in distributed mode in a Kafka Connect cluster
image:overview/kafka-concepts-kafka-connect.png[Kafka Connect worker interaction in distributed mode]

== Connectors

Connectors copy data from external systems to Kafka, or from Kafka into external systems.

* Source connectors push data into Kafka.
* Sink connectors extract data out of Kafka.

Plugins provide the implementation for Kafka Connect to run connector instances.
Connector instances create the tasks required to transfer data in and out of Kafka.
The Kafka Connect runtime orchestrates the tasks to split the work required between the worker pods.

MirrorMaker 2.0 also uses the Kafka Connect framework.
In this case, the external data system is another Kafka cluster.
Specialized connectors for MirrorMaker 2.0 manage data replication between source and target Kafka clusters.

[NOTE]
--
Kafka provides two built-in example connectors:

* `FileStreamSourceConnector` streams data from an external system to Kafka, reading lines from an input source and sending each line to a Kafka topic.
* `FileStreamSinkConnector` streams data from Kafka to an external system, reading messages from a Kafka topic and creating a line for each in an output file.
--

== Tasks

Data transfer orchestrated by the Kafka Connect runtime is split into tasks that run in parallel.
A task is started using the configuration supplied by a connector instance.
Kafka Connect distributes tasks to workers.

* A source connector task polls the external data system and returns a list of records that a worker sends to the Kafka brokers.
* A sink connector task receives Kafka records from a worker for writing to the external data system.

If a task fails, it can be restarted through the Kafka Connect API.

For sink connectors, the number of tasks created relates to the number of partitions being consumed.
For source connectors, how the source data is partitioned is defined by the connector.
You can control the maximum number of tasks that can run in parallel by setting `tasksMax` in the connector configuration.

== Workers

Workers store the initial connector configuration deployed to the Kafka Connect cluster.
They also start connectors and their tasks.

* Source connector workers read events from an external data system and use converters to construct key and value pairs to store in Kafka.
* Sink connector workers read records from Kafka and use converters to construct records to pass into an external data system.

A Kafka Connect cluster contains a group of workers with the same `group.id`.
The ID identifies the cluster within Kafka.
The ID is assigned in the worker configuration through the `KafkaConnect` resource.
Worker configuration also specifies the names of internal Kafka Connect topics.
The topics store connector offset and status information.
The group ID and names of these topics must also be unique to the Kafka Connect cluster.

Workers are assigned one or more connector instances and tasks.
The distributed approach to deploying Kafka Connect is fault tolerant and scalable.
If a worker pod fails, active workers share the tasks.
You can add to a group of worker pods through configuration of the `replicas` property in the `KafkaConnect` resource.

== Transforms

Kafka Connect translates and transforms external data.
Single-message transforms change messages into a format suitable for the target destination.
For example, a transform might insert or rename a field. Transforms can also filter and route data.
Plugins contain the implementation required for workers to perform one or more transformations.

* Source connectors transform data before converting it into a format supported by Kafka.
* Sink connectors transform data after converting it into a format suitable for an external data system.

A transform comprises a set of Java class files packaged in a JAR file for inclusion in a connector plugin.
Kafka Connect provides a set of standard transforms, but you can also create your own.

== Converters

When a worker receives data, it converts the data into an appropriate format using a converter.
You specify converters for workers in the worker `config` in the `KafkaConnect` resource.

Kafka Connect can convert data to and from formats supported by Kafka, such as JSON or Avro.
It also supports schemas for structuring data.
If you are not converting data into a structured format, you donâ€™t need to enable schemas.

NOTE: You can also specify converters for specific connectors to override the general Kafka Connect worker configuration that applies to all workers.

[role="_additional-resources"]
.Additional resources
* http://kafka.apache.org[Apache Kafka documentation^]
* link:{BookURLUsing}#property-kafka-connect-config-reference[Kafka Connect configuration of workers^] 
* link:{BookURLUsing}#proc-mirrormaker-replication-str[Synchronizing data between Kafka clusters using MirrorMaker 2.0^]
