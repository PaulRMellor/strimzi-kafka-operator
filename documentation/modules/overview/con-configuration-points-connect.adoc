// This module is included in:
//
// overview/assembly-configuration-points.adoc

[id="configuration-points-connect_{context}"]
= Kafka Connect configuration

[role="_abstract"]
You can use the `KafkaConnect` custom resource to configure a Kafka Connect instance.
Using this resource, you can configure the worker nodes that comprise a Kafka Connect cluster and build a container image with connector plugin configuration.
You can also enable KafkaConnectors in your `KafkaConnect` configuration to be able to use the `KafkaConnector` resource to manage connector plugins.

[discrete]
== Kafka Connect configuration
Use Strimzi’s `KafkaConnect` resource to quickly and easily create new Kafka Connect instances.
When you deploy Kafka Connect using the `KafkaConnect` resource, you specify bootstrap server addresses (`bootstrapServers`) for connecting to the Kafka cluster in its `spec`.
You can specify more than one address in case a server is down.
You also specify the authentication credentials and TLS encryption certificates to make a secure connection.

You specify the configuration for workers in the `config` of the `KafkaConnect` resource.

.Example worker configuration
[source,yaml,subs="attributes+"]
----
apiVersion: {KafkaConnectApiVersion}
kind: KafkaConnect
# ...
spec:
  config:
    group.id: my-connect-cluster <1>
    offset.storage.topic: my-connect-cluster-offsets <2>
    config.storage.topic: my-connect-cluster-configs <3>
    status.storage.topic: my-connect-cluster-status <4>
    Key.converter: org.apache.kafka.connect.json.JsonConverter <5>
    value.converter: org.apache.kafka.connect.json.JsonConverter <6>
    key.converter.schemas.enable: true <7>
    value.converter.schemas.enable: true <8>
    config.storage.replication.factor: 3 <9>
    offset.storage.replication.factor: 3 <10>
    status.storage.replication.factor: 3 <11>
  # ...
----
<1> Kafka Connect cluster group that the instance belongs to.
<2> Kafka topic that stores connector offsets.
<3> Kafka topic that stores connector and task status configurations.
<4> Kafka topic that stores connector and task status updates.
<5> Converter to transform message keys into JSON format for storage in Kafka.
<6> Converter to transform message values into JSON format for storage in Kafka.
<7> Schema enabled for converting message keys into structured JSON format.
<8> Schema enabled for converting message values into structured JSON format.
<9> Replication factor for the Kafka topic that stores connector offsets.
<10> Replication factor for the Kafka topic that stores connector and task status configurations.
<11> Replication factor for the Kafka topic that stores connector and task status updates.

NOTE: In this example, JSON converters are specified. The converters you specify must be in a data format supported by Kafka.

A distributed Kafka Connect cluster has a group ID and a set of internal connector topics.
Kafka Connect instances are configured by default with the same:

* Group ID for the Kafka Connect cluster
* Kafka topic to store the connector offsets
* Kafka topic to store connector and task status configurations
* Kafka topic to store connector and task status updates

If multiple different Kafka Connect instances are used, these settings must be unique for the workers of each Kafka Connect cluster created.

.Worker configuration that must be unique for each Kafka Connect instance
[source,yaml,subs="attributes+"]
----
apiVersion: {KafkaConnectApiVersion}
kind: KafkaConnect
# ...
spec:
  config:
    group.id: my-connect-cluster
    offset.storage.topic: my-connect-cluster-offsets
    config.storage.topic: my-connect-cluster-configs
    status.storage.topic: my-connect-cluster-status
----

The group ID identifies the Kafka Connect worker nodes. Kafka Connect clusters cannot share the ID or topics as it will create errors.
The names of the connectors used by each Kafka Connect instance must also be unique.

You can also deploy connector plugins using the `build` configuration of the `KafkaConnect` resource.
Connector plugins provide specific configuration for the creation of connector instances.

After connector plugins have been deployed, you can use Strimzi’s `KafkaConnector` custom resource or the Kafka Connect API to manage connector instances.
You can also create new connector instances using these options.

The Cluster Operator manages Kafka Connect clusters deployed using the `KafkaConnect` resource and connectors created using the `KafkaConnector` resource.

[discrete]
== Connector plugin configuration

Connector plugins provide the configuration for connecting to an external data source.
A set of JAR files or TGZ archives define the implementation required to connect to the data source.
Connector plugins are readily available for use with many external systems.
You can also create your own connector plugins.

Connector plugins provide the configuration as a set of files that define the implementation required to connect to specific types of external data system.
The configuration describes the source input data and target output data to feed into and out of Kafka Connect.
The external source data must reference specific topics that will store the messages.
The plugins might also contain the libraries and files needed to transform the data.

A Kafka Connect deployment can have one or more plugins, but only one version of each plugin.

You can create a custom Kafka Connect image that uses new connector plugins.
You can create the image in two ways:

* link:{BookURLDeploying}#creating-new-image-using-kafka-connect-build-str[Automatically using Kafka Connect configuration^]
* link:{BookURLDeploying}#creating-new-image-from-base-str[Manually using Dockerfile and a Kafka container image from {DockerRepository} as a base image^]

To create the container image automatically, you specify and deploy plugins with your Kafka Connect cluster using the `KafkaConnect` resource.
Strimzi automatically downloads and adds the plugin artifacts to a new container image.
The plugins are used by the workers that orchestrate the streaming processes.

Build (`build`) configuration automatically builds a container image with connector plugins.

.Example connector plugin configuration
[source,yaml,subs="attributes+"]
----
apiVersion: {KafkaConnectApiVersion}
kind: KafkaConnect
metadata:
  name: my-connect-cluster
  annotations:
    strimzi.io/use-connector-resources: "true"
spec:
  # ...
  build: <1>
    output: <2>
      type: docker
      image: my-registry.io/my-org/my-connect-cluster:latest
      pushSecret: my-registry-credentials
    plugins: <3>
      - name: debezium-postgres-connector
        artifacts:
          - type: tgz
            url: https://ARTIFACT-ADDRESS.tgz
            sha512sum: HASH-NUMBER-TO-VERIFY-ARTIFACT
      # ...
  # ...
----
<1> link:{BookURLUsing}#type-Build-reference[Build configuration properties^] for building a container image with connector plugins automatically.
<2> Configuration of the container registry where new images are pushed. The `output` properties describe the type and name of the image, and optionally the name of the Secret containing the credentials needed to access the container registry.
<3> List of connector plugins and their artifacts to add to the new container image. The `plugins` properties describe the type of artifact and the URL from which the artifact is downloaded. Each plugin must be configured with at least one artifact. Additionally, you can specify a SHA-512 checksum to verify the artifact before unpacking it.

If you are using a Dockerfile to build an image, you can use Strimzi’s latest container image as a base image to add your plugin configuration file.

.Example showing manual addition of connector plugin configuration
[source,subs="+quotes,attributes"]
----
FROM {DockerKafkaConnect}
USER root:root
COPY ./_my-plugins_/ /opt/kafka/plugins/
USER {DockerImageUser}
----

[discrete]
== `KafkaConnector` management of connectors

The `KafkaConnector` resource offers a Kubernetes-native approach to management of connectors by the Cluster Operator.
To manage connectors with `KafkaConnector` resources, you must specify an annotation in your `KafkaConnect` custom resource.

.Annotation to enable KafkaConnectors
[source,yaml,subs="attributes+"]
----
apiVersion: {KafkaConnectApiVersion}
kind: KafkaConnect
metadata:
  name: my-connect-cluster
  annotations:
    strimzi.io/use-connector-resources: "true"
  # ...
----

Setting `use-connector-resources` to `true`  enables KafkaConnectors to create, delete and reconfigure connectors.

If enabled in your `KafkaConnect` configuration, you must use the `KafkaConnector` resource to define connectors.
`KafkaConnector` resources are configured to connect to external systems.
They are deployed to the same Kubernetes cluster as the Kafka Connect cluster and Kafka cluster interacting with the external data system.

.Kafka components are contained in the same Kubernetes cluster
image:overview/kafka-concepts-kafka-connector.png[Kafka and Kafka Connect contained in the same cluster]

The configuration specifies how connector instances connect to an external system, including any authentication.
You also need to say what data to watch within the data source.
For example, you might provide a database name in the configuration.
You can also specify where the data should sit in Kafka by specifying a target topic name.

.Example KafkaConnector configuration
[source,yaml,subs="attributes+"]
----
apiVersion: {KafkaConnectApiVersion}
kind: KafkaConnector
metadata:
  name: my-source-connector  <1>
  labels:
    strimzi.io/cluster: my-connect-cluster <2>
spec:
  class: org.apache.kafka.connect.file.FileStreamSourceConnector <3>
  tasksMax: 2 <4>
  config: <5>
    file: "/opt/kafka/LICENSE" <6>
    topic: my-topic <7>
    # ...
----
<1> Name of the `KafkaConnector` resource, which is used as the name of the connector. Use any name that is valid for a Kubernetes resource.
<2> Name of the Kafka Connect cluster to create the connector instance in. Connectors must be deployed to the same namespace as the Kafka Connect cluster they link to.
<3> Full name or alias of the connector class. This should be present in the image being used by the Kafka Connect cluster.
<4> Maximum number of Kafka Connect tasks that the connector can create.
<5> link:{BookURLDeploying}#kafkaconnector-configs[Connector configuration^] as key-value pairs.
<6> Location of the connector configuration file. The source connector reads from the `/opt/kafka/LICENSE` file.
<7> Kafka topic to publish the source data to.

NOTE: You can link:{BookURLUsing}#proc-loading-config-with-provider-str[load confidential configuration values for a connector^] from Kubernetes Secrets or ConfigMaps.

[discrete]
== Kafka Connect API

Use the Kafka Connect REST API as an alternative to using the `KafkaConnector` custom resources to manage connectors.
The Kafka Connect REST API is available as a service running on `_CONNECT-CLUSTER-NAME_-connect-api:8083`, where _CONNECT-CLUSTER-NAME_ is the name of your Kafka Connect cluster.

You add the connector configuration as a JSON object.

.Example REST API curl request to add connector configuration
[source,curl,subs=attributes+]
----
curl -X POST \
  http://my-connect-cluster-connect-api:8083/connectors \
  -H 'Content-Type: application/json' \
  -d '{ "name": "my-source-connector",
    "config":
    {
      "class":"org.apache.kafka.connect.file.FileStreamSourceConnector",
      "file": "/opt/kafka/LICENSE",
      "topic":"my-topic",
      "tasksMax": "4",
      # ...
    }
}
'
----

If KafkaConnectors are enabled, manual changes made directly using the Kafka Connect REST API are reverted by the Cluster Operator.

The operations supported by the REST API are described in the http://kafka.apache.org[Apache Kafka documentation^].

[role="_additional-resources"]
.Additional resources

* link:{BookURLUsing}#assembly-kafka-connect-str[Kafka Connect configuration options^]
* link:{BookURLUsing}#con-kafka-connect-multiple-instances-str[Kafka Connect configuration for multiple instances^]
* link:{BookURLDeploying}#using-kafka-connect-with-plug-ins-str[Extending Kafka Connect with connector plug-ins^]
* link:{BookURLDeploying}#creating-new-image-using-kafka-connect-build-str[Creating a new container image automatically using Strimzi^]
* link:{BookURLDeploying}#creating-new-image-from-base-str[Creating a Docker image from the Kafka Connect base image^]
* link:{BookURLUsing}#type-Build-reference[Build schema reference^]
* link:{BookURLDeploying}#kafkaconnector-configs[Source and sink connector configuration options^]
* link:{BookURLUsing}#proc-loading-config-with-provider-str[Loading configuration values from external sources^]
