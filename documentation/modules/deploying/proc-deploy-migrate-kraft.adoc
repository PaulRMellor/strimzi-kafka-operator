// Module included in the following assemblies:
//
// deploying/deploying.adoc

[id='proc-deploy-migrate-kraft-{context}']
= Migrating to KRaft mode

[role="_abstract"]
If you are using ZooKeeper for metadata management in your Kafka cluster, you can migrate to using Kafka in KRaft mode. 
KRaft mode replaces ZooKeeper for distributed coordination, offering enhanced reliability, scalability, and throughput.

During the migration, you install a quorum of controller nodes as a node pool, which replaces ZooKeeper for management of your cluster. 
You enable KRaft migration in the cluster configuration by applying the `strimzi.io/kraft: migration` annotation.  
After the migration is complete, you switch the brokers to using KRaft and the controllers out of migration mode using the `strimzi.io/kraft: enabled` annotation.

Before starting the migration, verify that your environment can support Kafka in KRaft mode, as there are a number of xref:ref-operator-use-kraft-feature-gate-str[limitations].
Note also, the following:

* Migration is only supported on dedicated controller nodes, not on nodes with dual roles as brokers and controllers.
* Throughout the migration process, ZooKeeper and Controller nodes operate in parallel for a period, requiring sufficient compute resources in the cluster.

WARNING: ZooKeeper to KRaft migration is not yet suitable for production environments.

.Prerequisites

* You must be using Strimzi 0.40 or newer with Kafka 3.6.1 or newer. If you using an earlier version of Strimzi or APache Kafka, upgrade before migrating to KRaft mode.
* The Cluster Operator that manages the Kafka cluster is running.
* The Kafka cluster deployment uses Kafka node pools.
+
You can xref:proc-migrating-clusters-node-pools-str[migrate your existing Kafka cluster to use node pools]. 
+
In the `KafkaNodePool` resource configuration, brokers must be contained in a node pool assigned a `broker` role and with the name `kafka`.
Support for node pools is enabled in the `Kafka` resource configuration using the `strimzi.io/node-pools: enabled` annotation.

In this procedure, the Kafka cluster name is `my-cluster`, which is located in `my-project`. 
The name of the controller node pool is `controller`.

.Procedure

. For the Kafka cluster, create a node pool with a `controller` role.
+
Add a quorum of controller nodes to the node pool.
+
.Example configuration for a controller node pool
[source,yaml,subs="+attributes"]
----
apiVersion: {KafkaNodePoolApiVersion}
kind: KafkaNodePool
metadata:
  name: controller
  labels:
    strimzi.io/cluster: my-cluster
spec:
  replicas: 3
  roles:
    - controller
  storage:
    type: jbod
    volumes:
      - id: 0
        type: persistent-claim
        size: 20Gi
        deleteClaim: false
    resources:
      requests:
        memory: 64Gi
        cpu: "8"
      limits:
        memory: 64Gi
        cpu: "12"    
----
+
NOTE: For the migration, you cannot use a node pool of nodes that share the broker and controller roles.

. Apply the new `KafkaNodePool` resource to create the controllers.
+
Errors related to using controllers in a ZooKeeper-based environment are expected.

. Enable KRaft migration in the `Kafka` resource by setting the `strimzi.io/kraft` annotation to `migration`:
+
[source,shell]
----
kubectl annotate kafka my-cluster strimzi.io/kraft: migration
----
+
.Enabling KRaft migration
[source,yaml,subs="+attributes"]
----
apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
  namespace: my-project
  annotations:
    strimzi.io/kraft: migration 
# ...
----
+
Applying the annotation to the `Kafka` resource configuration starts the migration.

. Check the controllers have started and the brokers have rolled:
+
[source,shell]
----
kubectl get pods -n my-project
----
+
.Output shows nodes in broker and controller node pools
[source,shell]
----
NAME                     READY  STATUS   RESTARTS
my-cluster-broker-0      1/1    Running  0
my-cluster-broker-1      1/1    Running  0
my-cluster-broker-2      1/1    Running  0
my-cluster-controller-3  1/1    Running  0
my-cluster-controller-4  1/1    Running  0
my-cluster-controller-5  1/1    Running  0
# ...
----

. Check the status of the migration:
+
[source,shell]
----
kubectl get kafka my-cluster -n my-project -w
----
+
.Updates to the metadata state
[source,shell]
----
NAME        ...  METADATA STATE   WARNINGS
my-cluster  ...  Zookeeper
my-cluster  ...  KRaftMigration
my-cluster  ...  KRaftDualWriting
----
+
`METADATA STATE` shows the mechanism used to manage Kafka metadata and coordinate operations.
At the start of the migration this is `ZooKeeper`.
+
--
* `ZooKeeper` is the initial state when metadata is only stored in ZooKeeper.
* `KRaftMigration` is the state when the migration is in progress.
* `KRaftDualWriting` is the state when the migration has ended and the cluster is working as a KRaft cluster. 
Metadata is managed in Kafka but also replicated in ZooKeeper, so you can xref:proc-deploy-migrate-kraft-rollback-{context}[roll back from this point].
--
+
The migration status is also represented in the `status.kafkaMetadataState` property of the `Kafka` resource. 

. Enable KRaft in the `Kafka` resource configuration by setting the `strimzi.io/kraft` annotation to `enabled`:
+
[source,shell]
----
kubectl annotate kafka my-cluster strimzi.io/kraft: enabled
----
+
.Enabling KRaft migration
[source,yaml,subs="+attributes"]
----
apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
  namespace: my-project
  annotations:
    strimzi.io/kraft: enabled 
# ...
----
+
WARNING: Rollback cannot be performed after enabling KRaft.

. Check the status of the move to full KRaft mode:
+
[source,shell]
----
kubectl get kafka my-cluster -n my-project -w
----
+
.Updates to the metadata state
[source,shell]
----
NAME        ...  METADATA STATE        WARNINGS
my-cluster  ...  Zookeeper
my-cluster  ...  KRaftMigration
my-cluster  ...  KRaftDualWriting
my-cluster  ...  KRaftPostMigration
my-cluster  ...  KRaft                 True
----
+
`KRaftPostMigration` is the state (after the brokers have rolled) when KRaft mode is enabled and there is no ZooKeeper involvement, though ZooKeeper pods are still running.
`KRaft` is the final state (after the controllers have rolled) when the KRaft migration has finalized.
+
The warning (`True`) relates to ZooKeeper configuration being present in the `Kafka` resource configuration.
+
. Clean up the deployment to remove Zookeeper configuration and resources.

.. Remove `inter.broker.protocol.version`, `log.message.format.version`, and all `spec.zookeeper` configuration properties from the `Kafka` resource.
.. Apply the changes to the `Kafka` resource configuration.
.. Delete any resources related to the ZooKeeper deployment:
+
[source,shell]
----
kubectl delete <resource_type> <resource_name> -n my-project
----
+
For a list of resources created for ZooKeeper, see xref:ref-list-of-kafka-cluster-resources-str[].

[id='proc-deploy-migrate-kraft-rollback-{context}']
.Performing a rollback on the migration

Before the migration is finalized by enabling KRaft in the 'Kafka' resource, you can perform a rollback operation as follows:

. Apply the `strimzi.io/kraft: rollback` annotation to the `Kafka` resource to roll back the brokers and controllers.
+
[source,shell]
----
kubectl annotate kafka my-cluster strimzi.io/kraft: rollback
----
+
.Rolling back KRaft migration
[source,yaml,subs="+attributes"]
----
apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
  namespace: my-project
  annotations:
    strimzi.io/kraft: rollback 
# ...
----

. Delete the controllers node pool:
+
[source,shell]
----
kubectl delete KafkaNodePool controller -n my-project
----

. Apply the `strimzi.io/kraft: disabled` annotation to the `Kafka` resource to return the metadata state to `ZooKeeper`.
+
[source,shell]
----
kubectl annotate kafka my-cluster strimzi.io/kraft: disabled
----
+
.Switching back to using ZooKeeper
[source,yaml,subs="+attributes"]
----
apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
  namespace: my-project
  annotations:
    strimzi.io/kraft: disabled 
# ...
----
